{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNIwe5N7s0e_"
   },
   "source": [
    "# Real-world Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BG63Tpg8ep_"
   },
   "source": [
    "In this project, you will apply the skills you acquired in the course to gather and wrangle real-world data with two datasets of your choice.\n",
    "\n",
    "You will retrieve and extract the data, assess the data programmatically and visually, accross elements of data quality and structure, and implement a cleaning strategy for the data. You will then store the updated data into your selected database/data store, combine the data, and answer a research question with the datasets.\n",
    "\n",
    "Throughout the process, you are expected to:\n",
    "\n",
    "1. Explain your decisions towards methods used for gathering, assessing, cleaning, storing, and answering the research question\n",
    "2. Write code comments so your code is more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDYDkH-Zs7Nn"
   },
   "source": [
    "## 1. Gather data\n",
    "\n",
    "In this section, you will extract data using two different data gathering methods and combine the data. Use at least two different types of data-gathering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbN7z7rcuqpO"
   },
   "source": "### **1.1.** Problem Statement"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi6swhjSYqu2"
   },
   "source": [
    "I'd like to build the foundation for a historical weather dataset that is focused on Lander, Wyoming weather patterns. I'd like to identify how Climate Change has impacted my area in a particular.\n",
    "The challenge will be getting long term historical data combined with the most recent detailed data for in depth analysis. This can later be combined with emission data from the Department of Transportation, or Water Quality data from the State of Wyoming.\n",
    "The goal will be to store relevant metrics in normalized and cleaned form this will allow for further expansion by location, or metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AQfBAdUypMm"
   },
   "source": [
    "### **1.2.** Gather at least two datasets using two different data gathering methods\n",
    "\n",
    "List of data gathering methods:\n",
    "\n",
    "- Download data manually\n",
    "- Programmatically downloading files\n",
    "- Gather data by accessing APIs\n",
    "- Gather and extract data from HTML files using BeautifulSoup\n",
    "- Extract data from a SQL database\n",
    "\n",
    "Each dataset must have at least two variables, and have greater than 500 data samples within each dataset.\n",
    "\n",
    "For each dataset, briefly describe why you picked the dataset and the gathering method (2-3 full sentences), including the names and significance of the variables in the dataset. Show your work (e.g., if using an API to download the data, please include a snippet of your code). \n",
    "\n",
    "Load the dataset programmtically into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e6gS0wL1KTu"
   },
   "source": [
    "#### **Open Meteo - Hourly Soil and Radiation Weather Data**\n",
    "\n",
    "Type: Tabular records for hourly weather reporting in Lander Wyoming.\n",
    "\n",
    "Method: Gather data by accessing the Open Meteo Historical Weather Data API.\n",
    "\n",
    "The first dataset will come from the [Open Meteo API](https://open-meteo.com/en/docs/historical-forecast-api). Source information for this dataset can be found [here](https://open-meteo.com/en/docs/historical-forecast-api#data_sources).\n",
    "This archival API only has access to 2016 and beyond.\n",
    "\n",
    "Hourly Weather dataset variables:\n",
    "* **latitude**: float - latitude of the location\n",
    "* **longitude**: float - longitude of the location\n",
    "* **start_date**: datetime - start of hour (America/Denver)\n",
    "* **end_date**: datetime - end of hour (America/Denver)\n",
    "* **shortwave_radiation**: float - average temperature aggregated by 2m intervals (fahrenheit)\n",
    "* **direct_radiation**: float - sum of precipitation in (inch)\n",
    "* **diffuse_radiation**: float - amount of diffused radiation\n",
    "* **direct_normal_irradiance**: float - amount of direct normal irradiance\n",
    "* **global_tilted_irradiance**: float - amount of global tilted irradiance radiation\n",
    "* **terrestrial_radiation** - float - amount of terrestrial radiation\n",
    "* **soil_temperature_0cm**: float - temperature in C at 0cm\n",
    "* **soil_temperature_6cm**: float - temperature in C at 6cm\n",
    "* **soil_temperature_18cm**: float - temperature in C at 18cm\n",
    "* **soil_temperature_54cm**: float - temperature in C at 54cm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Su8E0uLuYkHU",
    "ExecuteTime": {
     "end_time": "2025-09-25T00:24:23.726586Z",
     "start_time": "2025-09-25T00:24:23.140555Z"
    }
   },
   "source": [
    "from weather_api_importer import get_hourly_weather_records_by_date, insert_hourly_weather_records\n",
    "from models import DailyWeatherRecord, NOAAStationMonthlySummary, Base, Location, MonthlyWeatherRecord, OMSolarHourlyWeatherRecord\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import insert, select, create_engine\n",
    "import numpy as np\n",
    "\n",
    "# Lander Wyoming Lat/Long values\n",
    "LATITUDE: float = 42.8330\n",
    "LONGITUDE: float = 108.7307\n",
    "START_DATE = \"2016-02-01\"\n",
    "END_DATE = \"2025-08-01\"\n",
    "ENGINE = create_engine(\"sqlite:///weather.db\")\n",
    "# Will be the default in future versions of Pandas\n",
    "pd.options.mode.copy_on_write = True"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T00:24:24.576571Z",
     "start_time": "2025-09-25T00:24:24.559833Z"
    }
   },
   "source": [
    "# Build the models within the SQLite Database\n",
    "Base.metadata.create_all(ENGINE)\n",
    "with ENGINE.begin() as conn:  # transactional context\n",
    "    default_location = conn.execute(\n",
    "        select(Location.id).where(\n",
    "            Location.latitude == LATITUDE,\n",
    "            Location.longitude == LONGITUDE,\n",
    "        )\n",
    "    ).scalar_one_or_none()\n",
    "\n",
    "    if default_location is None:\n",
    "        conn.execute(\n",
    "            insert(Location).values(\n",
    "                latitude=LATITUDE,\n",
    "                longitude=LONGITUDE,\n",
    "                friendly_name=\"Lander, Wyoming\",\n",
    "            )\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T00:24:54.706609Z",
     "start_time": "2025-09-25T00:24:53.367289Z"
    }
   },
   "source": [
    "# Identify if Open Meteo data should be imported\n",
    "# RUN the following IF importing fresh data\n",
    "# OMSolarHourlyWeatherRecord.__table__.drop(ENGINE, checkfirst=True)\n",
    "# OMSolarHourlyWeatherRecord.__table__.create(ENGINE, checkfirst=True)\n",
    "df_om_solar_hourly = pd.read_sql_table('om_solar_hourly_weather', ENGINE)\n",
    "if df_om_solar_hourly.empty:\n",
    "    hourly_records = get_hourly_weather_records_by_date(start_date=START_DATE, end_date=END_DATE)\n",
    "    insert_hourly_weather_records(hourly_records)\n",
    "\n",
    "df_om_solar_hourly = pd.read_sql_table('om_solar_hourly_weather', ENGINE)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T00:25:29.572038Z",
     "start_time": "2025-09-25T00:25:29.557966Z"
    }
   },
   "source": "df_om_solar_hourly.info()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83280 entries, 0 to 83279\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   id                        83280 non-null  int64         \n",
      " 1   location_id               83280 non-null  int64         \n",
      " 2   date                      83280 non-null  datetime64[ns]\n",
      " 3   shortwave_radiation       83280 non-null  float64       \n",
      " 4   direct_radiation          83280 non-null  float64       \n",
      " 5   diffuse_radiation         83280 non-null  float64       \n",
      " 6   direct_normal_irradiance  83280 non-null  float64       \n",
      " 7   global_tilted_irradiance  83280 non-null  float64       \n",
      " 8   soil_temperature_0cm      0 non-null      float64       \n",
      " 9   soil_temperature_6cm      0 non-null      float64       \n",
      " 10  soil_temperature_18cm     0 non-null      float64       \n",
      " 11  soil_temperature_54cm     0 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(9), int64(2)\n",
      "memory usage: 7.6 MB\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoUjq1tPzz7P"
   },
   "source": [
    "#### NOAA Monthly Summary\n",
    "\n",
    "Type: CSV File\n",
    "\n",
    "\n",
    "Method: From this [link](https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-month), filter the location to Lander Wyoming and download the full data set for 'LANDER AIRPORT, WY US (USW00024021.csv)'\n",
    "\n",
    "This DataSet was gathered from the National Centers for Environmental Information (NOAA), and is representative of the Lander Wyoming Weather data since 1948.\n",
    "More specifically this source is from the [Global Summary of the Month](https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-month) aggregate dataset.\n",
    "This data will be parsed and paired down to a usable dataset, and later stored to then compare with current trends from Data Set 1.\n",
    "\n",
    "The dataset has 110 columns to choose from, all of which use a shorthand so [this PDF](https://www.ncei.noaa.gov/pub/data/cdo/documentation/GSOM_documentation.pdf) can be used as a reference. Otherwise, these are the fields of interest that will be stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6zT0QxRyYmm7"
   },
   "source": [
    "df_noaa = pd.read_csv(\"./USWGlobalSummaryOfTheMonth.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df_noaa.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2 fields of interest\n",
    "* DATE - datetime\n",
    "* LATITUDE - float\n",
    "* LONGITUDE - float\n",
    "* NAME - string\n",
    "* ADPT - Monthly Average Dew Point Temperature\n",
    "* AWND - Monthly Average Wind Speed\n",
    "* CDSD - Cooling Degree Days (season-to-date)\n",
    "* DP01 - Number of days with >= 0.01 inch/0.254 millimeter in the month\n",
    "* DP1X - Number of days with >= 1.00 inch/25.4 millimeters in the month\n",
    "* DSND - Number of days with snow depth >= 1 inch/25 millimeters\n",
    "* DSNW - Number of days with snowfall >= 1 inch/25 millimeters\n",
    "* DT00 Number of days with maximum temperature <= 0 degrees Fahrenheit/-17.8 degrees Celsius\n",
    "* DT32 - Number of days with minimum temperature <= 32 degrees Fahrenheit/0 degrees Celsius\n",
    "* DX32 - Number of days with maximum temperature <= 32 degrees Fahrenheit/0 degrees Celsius\n",
    "* DX70 - Number of days with maximum temperature >= 70 degrees Fahrenheit/21.1 degrees Celsius\n",
    "* DX90 - Number of days with maximum temperature >= 90 degrees Fahrenheit/32.2 degrees Celsius\n",
    "* EMNT - Extreme minimum temperature for month. Lowest daily minimum temperature for the\n",
    "month\n",
    "* EMXP - Highest daily total of precipitation in the month. Given in inches or millimeters depending\n",
    "on user specification\n",
    "* EMXT - Extreme maximum temperature for month. Highest daily maximum temperature for the\n",
    "month. Given in Celsius or Fahrenheit depending on user specification\n",
    "* PRCP - Total Monthly Precipitation\n",
    "* PSUN - Monthly Average of the daily percents of possible sunshine\n",
    "* SNOW - Total Monthly Snowfall\n",
    "* TAVG - Average Monthly Temperature\n",
    "* TMAX - Monthly Maximum Temperature. Average of daily maximum temperature\n",
    "* TMIN - Monthly Minimum Temperature. Average of daily minimum temperature\n",
    "* TSUN - Daily total sunshine in minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assess data\n",
    "\n",
    "Assess the data according to data quality and tidiness metrics using the report below.\n",
    "\n",
    "List **two** data quality issues and **two** tidiness issues. Assess each data issue visually **and** programmatically, then briefly describe the issue you find.  **Make sure you include justifications for the methods you use for the assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue 1:\n",
    "Several columns are set as the incorrect datatype, and should be modified to meet their current representation of either a count of Days (int64), a float value, or string.\n",
    "Issue and justification:\n",
    "Not having the correct data types for these fields will lead to incorrect analysis down the line."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SpW59kh-zl8d"
   },
   "source": [
    "# The DF Hourly data is in good shape for null values and data types\n",
    "print(df_hourly.isnull().sum())\n",
    "print(df_hourly.isna().sum())\n",
    "print(df_hourly.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The date format is YYYY-MM and is an object type and should be setup as a datetime for later filtering of columns\n",
    "print(df_noaa.dtypes)\n",
    "df_noaa[\"DATE\"].dtype"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue 2:\n",
    "The NOAA Dataset has a large number of null values for the chosen location. Sort through these and drop ones that are primarily null."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_noaa.isnull().sum().sort_values(ascending=False)\n",
    "# I suspect the missing values may be related to dates"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXhGiYyiwwKN"
   },
   "source": [
    "### Tidiness Issue 1:\n",
    "Given that we know the location is Lander Wyoming we can drop that data and replace it with location id = 1 prior to storing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fleC5rORI0Xl"
   },
   "source": [
    "# FILL IN - Inspecting the dataframe visually\n",
    "df_noaa[['NAME', 'LATITUDE', 'LONGITUDE', 'STATION', 'ELEVATION']].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffMoRGSwzYj"
   },
   "source": [
    "### Tidiness Issue 2:\n",
    "The incoming data is setup as hourly data, this is great for granularity, but to compare to the monthly summary we will need to build up a daily and weekly set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XUpeoqokw5Qt"
   },
   "source": [
    "df_hourly.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c8JK4DoXxtFA"
   },
   "source": [
    "df_hourly.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6gmLnBttpCh"
   },
   "source": [
    "## 3. Clean data\n",
    "Clean the data to solve the 4 issues corresponding to data quality and tidiness found in the assessing step. **Make sure you include justifications for your cleaning decisions.**\n",
    "\n",
    "After the cleaning for each issue, please use **either** the visually or programatical method to validate the cleaning was succesful.\n",
    "\n",
    "At this stage, you are also expected to remove variables that are unnecessary for your analysis and combine your datasets. Depending on your datasets, you may choose to perform variable combination and elimination before or after the cleaning stage. Your dataset must have **at least** 4 variables after combining the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_hourly_copy = df_hourly.copy()\n",
    "df_noaa_copy = df_noaa.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmFhN52Yyn3l"
   },
   "source": [
    "### **Quality Issue 1:**\n",
    "Setup as a proper types for each column (based on the aforementioned PDF), then abstract out a year and month column for later grouping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9UejDWrNMW4a"
   },
   "source": [
    "df_noaa_copy.loc[:, \"year\"] = pd.Series(\n",
    "    [dt[1].split(\"-\")[0] for dt in df_noaa_copy[\"DATE\"].items()]\n",
    ")\n",
    "df_noaa_copy.loc[:, \"month\"] = pd.Series(\n",
    "    [dt[1].split(\"-\")[1] for dt in df_noaa_copy[\"DATE\"].items()]\n",
    ")\n",
    "df_noaa_copy.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure data integrity prior to conversion, ignoring attributes and non-essential columns\n",
    "int_cols = ['DP01','DP10','DP1X','DSND','DSNW','DT00','DT32','DX32','DX70','DX90','DYFG','DYHF','DYNT','DYSD','DYSN','DYTS','DYXP','DYXT','WDF2','WDF5']\n",
    "float_cols = ['CDSD','CLDD','EMNT','EMSD','EMSN','EMXP','EMXT','HDSD','HTDD','PRCP','SNOW','TAVG','TMAX','TMIN','WSF2','WSF5']\n",
    "\n",
    "for col in int_cols:\n",
    "    if col in df_noaa_copy.columns:\n",
    "        df_noaa_copy[col] = pd.to_numeric(df_noaa_copy[col], errors='coerce').astype('Int64')\n",
    "\n",
    "for col in float_cols:\n",
    "    if col in df_noaa_copy.columns:\n",
    "        df_noaa_copy[col] = pd.to_numeric(df_noaa_copy[col], errors='coerce').astype(float)\n",
    "\n",
    "# Ensure date is datetime64[ns] representing the first day of the month\n",
    "df_noaa_copy['date'] = pd.to_datetime(df_noaa_copy['DATE'], format='%Y-%m', errors='raise')\n",
    "\n",
    "# Inspect dtypes after coercion\n",
    "df_noaa_copy.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_DAUbJrymBL"
   },
   "source": [
    "### **Quality Issue 2**\n",
    "Identifying null columns and dropping ones that exceed a reasonable threshold. If a pattern develops by year we could shorten the time span for comparision."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure we filter out the _ATTRIBUTE columns to shorten the width of this df\n",
    "df_noaa_copy = df_noaa_copy[[col for col in df_noaa_copy.columns if not col.endswith('_ATTRIBUTES')]]\n",
    "\n",
    "# Count total rows per year (denominator)\n",
    "rows_per_year = df_noaa_copy.groupby(\"year\").size()\n",
    "\n",
    "# Sum of nulls per year by selected columns\n",
    "nulls_by_year = df_noaa_copy.groupby(\"year\")[df_noaa_copy.columns].apply(\n",
    "    lambda g: g.isnull().sum()\n",
    ")\n",
    "\n",
    "# Convert to proportions\n",
    "null_proportion_by_year = (nulls_by_year.div(rows_per_year, axis=0) * 100).round(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "ax = sns.heatmap(\n",
    "    null_proportion_by_year,\n",
    "    annot=False,\n",
    "    cmap='Greens',\n",
    "    vmin=0, vmax=100,\n",
    "    linewidths=0.5, linecolor='white',\n",
    "    cbar_kws={'label': '% null', 'format': '%.0f%%'}\n",
    ")\n",
    "plt.title('Proportion Nulls by Year (Null Columns)')\n",
    "plt.ylabel('Year')\n",
    "plt.xlabel('Column')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Using the heatmap above I can visually see where some fields are null and at which year they are concentrated.\n",
    "# To clean up this chart we filter out columns that won't be useful for analysis (null columns), and include fields of interest\n",
    "try:\n",
    "    df_noaa_copy.drop(['ADPT', 'ASLP', 'ASTP', 'AWBT', 'AWND', 'PSUN', 'RHAV', 'RHMN', 'RHMX', 'TSUN', 'WDF1', 'WDFG', 'WDFM', 'WSF1', 'WSFG', 'WSFM'], axis=1, inplace=True)\n",
    "except KeyError:\n",
    "    # Already dropped these columns\n",
    "    pass\n",
    "rows_per_year = df_noaa_copy.groupby(\"year\").size()\n",
    "nulls_by_year = df_noaa_copy.groupby(\"year\")[df_noaa_copy.columns].apply(\n",
    "    lambda g: g.isnull().sum()\n",
    ")\n",
    "null_proportion_by_year = (nulls_by_year.div(rows_per_year, axis=0) * 100).round(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ionB2sRaMUmY"
   },
   "source": [
    "# Rebuild the same heatmap to determine the concentration of columns that have values\n",
    "plt.figure(figsize=(18, 9))\n",
    "ax = sns.heatmap(\n",
    "    null_proportion_by_year,\n",
    "    annot=False,\n",
    "    cmap='Greens',\n",
    "    vmin=0, vmax=100,\n",
    "    linewidths=0.5, linecolor='white',\n",
    "    cbar_kws={'label': '% null', 'format': '%.0f%%'}\n",
    ")\n",
    "plt.title('Proportion Nulls by Year (Top Null Columns)')\n",
    "plt.ylabel('Year')\n",
    "plt.xlabel('Column')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now the Heatmap is looking a lot cleaner. We will have to keep in mind there are some gaps with the WDF fields, but since it is bound by date (>1996) we can work around that."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIUrrfSNyOPR"
   },
   "source": [
    "### **Tidiness Issue 1**\n",
    "Prior to storing we will want to bring the location data into normal form via the Location table found in models.py"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_noaa_copy.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We can remove the station\n",
    "current_columns = df_noaa_copy.columns\n",
    "if 'STATION' in current_columns:\n",
    "    df_noaa_copy.drop('STATION', axis=1, inplace=True)\n",
    "if 'LONGITUDE' in current_columns:\n",
    "    df_noaa_copy.drop('LONGITUDE', axis=1, inplace=True)\n",
    "if 'LATITUDE' in current_columns:\n",
    "    df_noaa_copy.drop('LATITUDE', axis=1, inplace=True)\n",
    "if 'NAME' in current_columns:\n",
    "    df_noaa_copy.drop('NAME', axis=1, inplace=True)\n",
    "if 'ELEVATION' in current_columns:\n",
    "    df_noaa_copy.drop('ELEVATION', axis=1, inplace=True)\n",
    "if 'location_id' not in current_columns:\n",
    "    df_noaa_copy.loc[:, 'location_id'] = 1\n",
    "df_noaa_copy.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Justification: By simplifying down to a normal form location table we can expand this to join to other data sets by location and date in the future."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o51Bt8kwyTzk"
   },
   "source": [
    "### **Tidiness Issue 2**\n",
    "In order to compare the hourly data more efectively we will want to roll it up into a daily record table."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_hourly_copy[\"day\"] = df_hourly_copy[\"date\"].dt.normalize()\n",
    "df_hourly_copy['month'] = df_hourly_copy['date'].dt.month\n",
    "df_hourly_copy['year'] = df_hourly_copy['date'].dt.year\n",
    "df_hourly_copy.head()\n",
    "# Add in proper date formatting"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by day and compute aggregates\n",
    "df_agg_daily = (\n",
    "    df_hourly_copy.groupby(\"day\").agg(\n",
    "        average_temperature=(\"temperature\", \"mean\"),\n",
    "        min_temperature=(\"temperature\", \"min\"),\n",
    "        max_temperature=(\"temperature\", \"max\"),\n",
    "        average_wind_speed=(\"wind_speed\", \"mean\"),\n",
    "        min_wind_speed=(\"wind_speed\", \"min\"),\n",
    "        max_wind_speed=(\"wind_speed\", \"max\"),\n",
    "        precipitation_sum=(\"precipitation\", \"sum\"),\n",
    "        precipitation_min=(\"precipitation\", \"min\"),\n",
    "        precipitation_max=(\"precipitation\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"day\": \"date_time\"})\n",
    ")\n",
    "df_agg_daily.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_agg_monthly = (\n",
    "    df_hourly_copy.groupby(['month', 'year']).agg(\n",
    "        average_temperature=(\"temperature\", \"mean\"),\n",
    "        min_temperature=(\"temperature\", \"min\"),\n",
    "        max_temperature=(\"temperature\", \"max\"),\n",
    "        average_wind_speed=(\"wind_speed\", \"mean\"),\n",
    "        min_wind_speed=(\"wind_speed\", \"min\"),\n",
    "        max_wind_speed=(\"wind_speed\", \"max\"),\n",
    "        precipitation_sum=(\"precipitation\", \"sum\"),\n",
    "        precipitation_min=(\"precipitation\", \"min\"),\n",
    "        precipitation_max=(\"precipitation\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "df_agg_monthly"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add calendar columns and location_id to Daily column\n",
    "df_agg_daily[\"month\"] = df_agg_daily[\"date_time\"].dt.month.astype(int)\n",
    "df_agg_daily[\"day_of_month\"] = df_agg_daily[\"date_time\"].dt.day.astype(int)\n",
    "df_agg_daily[\"year\"] = df_agg_daily[\"date_time\"].dt.year.astype(int)\n",
    "df_agg_daily[\"location_id\"] = 1 # Current default location is 1 for Lander, Wyoming\n",
    "# Add location ID and month column to monthly DF\n",
    "df_agg_monthly[\"location_id\"] = 1 # Current default location is 1 for Lander, Wyoming\n",
    "df_agg_monthly['date'] = df_agg_monthly['year'].astype(str) + \"-\" + df_agg_monthly['month'].astype(str)\n",
    "# Will default to the first day of the month\n",
    "df_agg_monthly['date'] = pd.to_datetime(df_agg_monthly['date'])\n",
    "df_agg_monthly['date'].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q6I_Sr7lxXi5"
   },
   "source": "df_agg_monthly",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove unnecessary variables and combine datasets**\n",
    "\n",
    "Depending on the datasets, you can also peform the combination before the cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ensure column ordering matches models.py file\n",
    "df_agg_daily_final = df_agg_daily[\n",
    "    [\n",
    "        \"location_id\",\n",
    "        \"date_time\",\n",
    "        \"month\",\n",
    "        \"day_of_month\",\n",
    "        \"year\",\n",
    "        \"average_temperature\",\n",
    "        \"min_temperature\",\n",
    "        \"max_temperature\",\n",
    "        \"average_wind_speed\",\n",
    "        \"min_wind_speed\",\n",
    "        \"max_wind_speed\",\n",
    "        \"precipitation_sum\",\n",
    "        \"precipitation_min\",\n",
    "        \"precipitation_max\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "df_agg_monthly_final = df_agg_monthly[\n",
    "    [\n",
    "        \"location_id\",\n",
    "        \"date\",\n",
    "        \"month\",\n",
    "        \"year\",\n",
    "        \"average_temperature\",\n",
    "        \"min_temperature\",\n",
    "        \"max_temperature\",\n",
    "        \"average_wind_speed\",\n",
    "        \"min_wind_speed\",\n",
    "        \"max_wind_speed\",\n",
    "        \"precipitation_sum\",\n",
    "        \"precipitation_min\",\n",
    "     ]\n",
    "]\n",
    "\n",
    "df_noaa_final = df_noaa_copy[\n",
    "    ['location_id', 'date', 'CDSD', 'CLDD', 'DP01', 'DP10', 'DP1X', 'DSND', 'DSNW', 'DT00',\n",
    "       'DT32', 'DX32', 'DX70', 'DX90', 'DYFG', 'DYHF', 'DYNT', 'DYSD', 'DYSN',\n",
    "       'DYTS', 'DYXP', 'DYXT', 'EMNT', 'EMSD', 'EMSN', 'EMXP', 'EMXT', 'HDSD',\n",
    "       'HTDD', 'PRCP', 'SNOW', 'TAVG', 'TMAX', 'TMIN', 'WDF2', 'WDF5', 'WSF2',\n",
    "       'WSF5']\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F42urHuzttjF"
   },
   "source": [
    "## 4. Update your data store\n",
    "Update your local database/data store with the cleaned data, following best practices for storing your cleaned data:\n",
    "\n",
    "- Must maintain different instances / versions of data (raw and cleaned data)\n",
    "- Must name the dataset files informatively\n",
    "- Ensure both the raw and cleaned data is saved to your database/data store"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clear out any existing records in the daily_weather table\n",
    "DailyWeatherRecord.__table__.drop(ENGINE, checkfirst=True)\n",
    "DailyWeatherRecord.__table__.create(ENGINE, checkfirst=True)\n",
    "MonthlyWeatherRecord.__table__.drop(ENGINE, checkfirst=True)\n",
    "MonthlyWeatherRecord.__table__.create(ENGINE, checkfirst=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V3uay7EJUV_L"
   },
   "source": [
    "daily_records = df_agg_daily_final.to_dict(orient=\"records\")\n",
    "monthly_records = df_agg_monthly_final.to_dict(orient=\"records\")\n",
    "daily_stmt = insert(DailyWeatherRecord)\n",
    "monthly_stmt = insert(MonthlyWeatherRecord)\n",
    "with ENGINE.begin() as conn:\n",
    "    for _d in daily_records:\n",
    "        conn.execute(daily_stmt, _d)\n",
    "    for _m in monthly_records:\n",
    "        conn.execute(monthly_stmt, _m)\n",
    "    conn.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop the table prior to import\n",
    "NOAAStationMonthlySummary.__table__.drop(ENGINE, checkfirst=True)\n",
    "NOAAStationMonthlySummary.__table__.create(ENGINE, checkfirst=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Store the records for later retrieval\n",
    "records = df_noaa_final.to_dict(orient=\"records\")\n",
    "stmt = insert(NOAAStationMonthlySummary)\n",
    "with ENGINE.begin() as conn:\n",
    "    for record in records:\n",
    "        conn.execute(stmt, record)\n",
    "    conn.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combine the datasets for analysis\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fetch the cleaned date for merging\n",
    "_df_noaa_monthly_summary = pd.read_sql_table('noaa_monthly_summary', ENGINE)\n",
    "_df_monthly = pd.read_sql_table('monthly_weather', ENGINE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_df_noaa_monthly_summary.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_df_monthly.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure proper date format prior to merge\n",
    "_df_noaa_monthly_summary['date'] = _df_noaa_monthly_summary['date'].dt.normalize()\n",
    "_df_noaa_monthly_summary['date'].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_df_monthly['date'] = _df_monthly['date'].dt.normalize()\n",
    "_df_monthly['date'].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove Unnecessary fields\n",
    "_df_monthly_final = _df_monthly.drop(['id', 'location_id', 'month', 'year'], axis=1)\n",
    "_df_monthly_final.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge the dataframes\n",
    "df_merged_monthly_summary = pd.merge(_df_noaa_monthly_summary, _df_monthly_final, on='date', how='left')\n",
    "df_merged_monthly_summary.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGy_yddGtzhM"
   },
   "source": [
    "## 5. Answer the research question\n",
    "\n",
    "### **5.1:** Define and answer the research question \n",
    "Going back to the problem statement in step 1, use the cleaned data to answer the question you raised. Produce **at least** two visualizations using the cleaned data and explain how they help you answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjedE4s4ZkEd"
   },
   "source": "*Research question:* How has Lander Wyoming been impacted by Climate Change?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_merged_monthly_summary['year'] = df_merged_monthly_summary['date'].dt.year\n",
    "df_merged_monthly_summary['month'] = df_merged_monthly_summary['date'].dt.month\n",
    "df_merged_monthly_summary.info()\n",
    "# TODO: Investigate precip max"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visual 1 - Month over Month percentage change of Max/Min and Avg temperature\n",
    "# Compute annual mean of TMAX\n",
    "annual_temperature = (\n",
    "    a.groupby('year', as_index=False)['TMAX']\n",
    "     .mean()\n",
    "     .rename(columns={'TMAX': 'tmax_mean'})\n",
    ")\n",
    "\n",
    "# Year-over-year absolute change\n",
    "annual_tmax['yoy_change'] = annual_tmax['tmax_mean'].diff()\n",
    "\n",
    "# Plot YoY change as a bar chart with up/down colors\n",
    "colors = ['green' if v >= 0 else 'red' for v in annual_tmax['yoy_change'].fillna(0)]\n",
    "ax = annual_tmax.plot(x='year', y='yoy_change', kind='bar', color=colors, legend=False, figsize=(18, 5))\n",
    "ax.set_title('Year-over-Year Change in TMAX (Annual Mean)')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Change in TMAX')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to research question:*\n",
    "As we can see there is some variability year over year, but the average maximum temperature has been trending upwards in Lander Wyoming. Next let's get a cleaner view and see about pinpointing by how much the temperature is trending upwards."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6fdK_8ZGZm9R"
   },
   "source": [
    "# Visual 2 - Annual average temperature trend (TAVG) with 10-year rolling mean and linear trend\n",
    "b = _df_noaa\n",
    "annual_tavg = (\n",
    "    b.groupby('year', as_index=False)['TAVG']\n",
    "      .mean()\n",
    "      .rename(columns={'TAVG': 'tavg_mean'})\n",
    "      .sort_values('year')\n",
    ")\n",
    "# 10-year rolling mean (centered)\n",
    "annual_tavg['roll10'] = annual_tavg['tavg_mean'].rolling(10, min_periods=5, center=True).mean()\n",
    "# Linear trend\n",
    "x = annual_tavg['year'].to_numpy()\n",
    "y = annual_tavg['tavg_mean'].to_numpy()\n",
    "# Must have more than 2 years to build trend line\n",
    "if len(annual_tavg) >= 2:\n",
    "    coef = np.polyfit(x, y, 1)\n",
    "    trend_fn = np.poly1d(coef)\n",
    "    annual_tavg['trend'] = trend_fn(x)\n",
    "    slope_decade = coef[0] * 10.0\n",
    "else:\n",
    "    annual_tavg['trend'] = np.nan\n",
    "    slope_decade = float('nan')\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(annual_tavg['year'], annual_tavg['tavg_mean'], color='lightgray', linewidth=2, label='Annual mean (TAVG)')\n",
    "plt.plot(annual_tavg['year'], annual_tavg['roll10'], color='steelblue', linewidth=3, label='10-yr rolling mean')\n",
    "\n",
    "if np.isfinite(slope_decade):\n",
    "    plt.plot(annual_tavg['year'], annual_tavg['trend'], color='crimson', linestyle='--', linewidth=2, label='Linear trend')\n",
    "    plt.title('Annual Average Temperature (TAVG) with 10-year Rolling Mean and Linear Trend')\n",
    "    print(f'Estimated warming trend: {slope_decade:.2f} °C per decade')\n",
    "else:\n",
    "    plt.title('Annual Average Temperature (TAVG) with 10-year Rolling Mean')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5RgvMGUZoHn"
   },
   "source": [
    "*Answer to research question:*\n",
    "To further suggest the increase in temperature year over year we can see in the above chart that the average temperature has been trending hotter by around 0.1 degrees C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ezWXXZVj-TP"
   },
   "source": [
    "### **5.2:** Reflection\n",
    "In 2-4 sentences, if you had more time to complete the project, what actions would you take? For example, which data quality and structural issues would you look into further, and what research questions would you further explore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB3RBDG5kFe1"
   },
   "source": [
    "*Answer:*\n",
    "Given more time I would like to investigate the trends as of late, and how they compare to events around the world. For example hurricanes, Cloud Seeding, and changes in emissions output or policy.\n",
    "Given more time I would like to investigate further trends in the wide assortment of variables present in the Open Meteo, and NOAA data sets. By having these sets parsed aggregated and in a local database I will be able to build on this project as time goes on."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
