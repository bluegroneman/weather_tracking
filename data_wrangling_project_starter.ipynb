{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNIwe5N7s0e_"
   },
   "source": [
    "# Real-world Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BG63Tpg8ep_"
   },
   "source": [
    "In this project, you will apply the skills you acquired in the course to gather and wrangle real-world data with two datasets of your choice.\n",
    "\n",
    "You will retrieve and extract the data, assess the data programmatically and visually, accross elements of data quality and structure, and implement a cleaning strategy for the data. You will then store the updated data into your selected database/data store, combine the data, and answer a research question with the datasets.\n",
    "\n",
    "Throughout the process, you are expected to:\n",
    "\n",
    "1. Explain your decisions towards methods used for gathering, assessing, cleaning, storing, and answering the research question\n",
    "2. Write code comments so your code is more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDYDkH-Zs7Nn"
   },
   "source": [
    "## 1. Gather data\n",
    "\n",
    "In this section, you will extract data using two different data gathering methods and combine the data. Use at least two different types of data-gathering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbN7z7rcuqpO"
   },
   "source": "### **1.1.** Problem Statement"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi6swhjSYqu2"
   },
   "source": [
    "I'd like to build the foundation for a historical weather dataset that is focused on Lander, Wyoming weather patterns. I'd like to identify how Climate Change has impacted my area in a particular.\n",
    "The challenge will be getting long term historical data combined with the most recent detailed data for in depth analysis. This can later be combined with emission data from the Department of Transportation, or Water Quality data from the State of Wyoming.\n",
    "The goal will be to build this into a normalized database and add re-usable methods for fetching and storing the data so the reports could be expanded to other locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AQfBAdUypMm"
   },
   "source": [
    "### **1.2.** Gather at least two datasets using two different data gathering methods\n",
    "\n",
    "List of data gathering methods:\n",
    "\n",
    "- Download data manually\n",
    "- Programmatically downloading files\n",
    "- Gather data by accessing APIs\n",
    "- Gather and extract data from HTML files using BeautifulSoup\n",
    "- Extract data from a SQL database\n",
    "\n",
    "Each dataset must have at least two variables, and have greater than 500 data samples within each dataset.\n",
    "\n",
    "For each dataset, briefly describe why you picked the dataset and the gathering method (2-3 full sentences), including the names and significance of the variables in the dataset. Show your work (e.g., if using an API to download the data, please include a snippet of your code). \n",
    "\n",
    "Load the dataset programmtically into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e6gS0wL1KTu"
   },
   "source": [
    "#### **Hourly Weather Data**\n",
    "\n",
    "Type: Tabular records for hourly weather reporting in Lander Wyoming.\n",
    "\n",
    "Method: Gather data by accessing the Open Meteo Historical Weather Data API.\n",
    "\n",
    "The first dataset will come from the [Open Meteo API](https://open-meteo.com/en/docs/historical-forecast-api). Source information for this dataset can be found [here](https://open-meteo.com/en/docs/historical-forecast-api#data_sources).\n",
    "This archival API only has access to 2016 and beyond.\n",
    "\n",
    "Hourly Weather dataset variables:\n",
    "* latitude: float - latitude of the location\n",
    "* longitude: float - longitude of the location\n",
    "* start_date: datetime - start of hour (America/Denver)\n",
    "* end_date: datetime - end of hour (America/Denver)\n",
    "* temperature_2m: float - average temperature aggregated by 2m intervals (fahrenheit)\n",
    "* precipitation: float - sum of precipitation in (inch)\n",
    "* wind_speed_10m: float - average wind speed aggregated by 10m intervals (mph)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Su8E0uLuYkHU"
   },
   "source": [
    "from main import migrate, import_weather_data\n",
    "from models import DailyWeatherRecord, NOAAStationMonthlySummary\n",
    "import pandas as pd\n",
    "from constants import ENGINE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import insert\n",
    "\n",
    "IMPORT = False\n",
    "pd.options.mode.copy_on_write = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build the weather data set\n",
    "migrate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import the weather data from open meteo API archive\n",
    "if IMPORT:\n",
    "    import_weather_data(start_date=\"2017-01-01\", end_date=\"2025-08-01\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_hourly = pd.read_sql_table(\"hourly_weather\", ENGINE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df_hourly.shape",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoUjq1tPzz7P"
   },
   "source": [
    "#### NOAA Monthly Summary\n",
    "\n",
    "Type: CSV File\n",
    "\n",
    "\n",
    "Method: From this [link](https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-month), filter the location to Lander Wyoming and download the full data set for 'LANDER AIRPORT, WY US (USW00024021.csv)'\n",
    "\n",
    "This DataSet was gathered from the National Centers for Environmental Information (NOAA), and is representative of the Lander Wyoming Weather data since 1948.\n",
    "More specifically this source is from the [Global Summary of the Month](https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-month) aggregate dataset.\n",
    "This data will be parsed and paired down to a usable dataset, and later stored to then compare with current trends from Data Set 1.\n",
    "\n",
    "The dataset has 110 columns to choose from, all of which use a shorthand so [this PDF](https://www.ncei.noaa.gov/pub/data/cdo/documentation/GSOM_documentation.pdf) can be used as a reference. Otherwise, these are the fields of interest that will be stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6zT0QxRyYmm7"
   },
   "source": [
    "df_noaa = pd.read_csv(\"./USWGlobalSummaryOfTheMonth.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df_noaa.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2 fields of interest\n",
    "* DATE - datetime\n",
    "* LATITUDE - float\n",
    "* LONGITUDE - float\n",
    "* NAME - string\n",
    "* ADPT - Monthly Average Dew Point Temperature\n",
    "* AWND - Monthly Average Wind Speed\n",
    "* CDSD - Cooling Degree Days (season-to-date)\n",
    "* DP01 - Number of days with >= 0.01 inch/0.254 millimeter in the month\n",
    "* DP1X - Number of days with >= 1.00 inch/25.4 millimeters in the month\n",
    "* DSND - Number of days with snow depth >= 1 inch/25 millimeters\n",
    "* DSNW - Number of days with snowfall >= 1 inch/25 millimeters\n",
    "* DT00 Number of days with maximum temperature <= 0 degrees Fahrenheit/-17.8 degrees Celsius\n",
    "* DT32 - Number of days with minimum temperature <= 32 degrees Fahrenheit/0 degrees Celsius\n",
    "* DX32 - Number of days with maximum temperature <= 32 degrees Fahrenheit/0 degrees Celsius\n",
    "* DX70 - Number of days with maximum temperature >= 70 degrees Fahrenheit/21.1 degrees Celsius\n",
    "* DX90 - Number of days with maximum temperature >= 90 degrees Fahrenheit/32.2 degrees Celsius\n",
    "* EMNT - Extreme minimum temperature for month. Lowest daily minimum temperature for the\n",
    "month\n",
    "* EMXP - Highest daily total of precipitation in the month. Given in inches or millimeters depending\n",
    "on user specification\n",
    "* EMXT - Extreme maximum temperature for month. Highest daily maximum temperature for the\n",
    "month. Given in Celsius or Fahrenheit depending on user specification\n",
    "* PRCP - Total Monthly Precipitation\n",
    "* PSUN - Monthly Average of the daily percents of possible sunshine\n",
    "* SNOW - Total Monthly Snowfall\n",
    "* TAVG - Average Monthly Temperature\n",
    "* TMAX - Monthly Maximum Temperature. Average of daily maximum temperature\n",
    "* TMIN - Monthly Minimum Temperature. Average of daily minimum temperature\n",
    "* TSUN - Daily total sunshine in minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assess data\n",
    "\n",
    "Assess the data according to data quality and tidiness metrics using the report below.\n",
    "\n",
    "List **two** data quality issues and **two** tidiness issues. Assess each data issue visually **and** programmatically, then briefly describe the issue you find.  **Make sure you include justifications for the methods you use for the assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue 1:\n",
    "The DATE column is in the incorrect datatype, and it would be beneficial to have a year and month column for better grouping analysis\n",
    "Issue and justification:\n",
    "Not having values for these fields means they will not be useful for analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SpW59kh-zl8d"
   },
   "source": [
    "# The DF Hourly data is in good shap for null values\n",
    "df_hourly.isnull().sum()\n",
    "df_hourly.isna().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The date format is YYYY-MM and is an object type, in order to be able to sort dates there should be a year and month column\n",
    "df_noaa[\"DATE\"].dtype"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue 2:\n",
    "The NOAA Dataset has a large number of null values for the chosen location. Sort through these and drop ones that are mostly null >=50%"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The NOAA dataset has a large number of null values\n",
    "df_noaa.isnull().sum().sort_values(ascending=False)\n",
    "# I suspect the missing values may be related to dates"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXhGiYyiwwKN"
   },
   "source": [
    "### Tidiness Issue 1:\n",
    "Given that we know the location is Lander Wyoming we can drop that data and replace it with location id = 1 prior to storing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fleC5rORI0Xl"
   },
   "source": [
    "# FILL IN - Inspecting the dataframe visually\n",
    "df_noaa[['NAME', 'LATITUDE', 'LONGITUDE', 'STATION', 'ELEVATION']].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffMoRGSwzYj"
   },
   "source": [
    "### Tidiness Issue 2:\n",
    "The incoming data is setup as hourly data, this is great for granularity, but to compare to the monthly summary we will need to build up a daily and weekly set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XUpeoqokw5Qt"
   },
   "source": [
    "df_hourly.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c8JK4DoXxtFA"
   },
   "source": [
    "df_hourly.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6gmLnBttpCh"
   },
   "source": [
    "## 3. Clean data\n",
    "Clean the data to solve the 4 issues corresponding to data quality and tidiness found in the assessing step. **Make sure you include justifications for your cleaning decisions.**\n",
    "\n",
    "After the cleaning for each issue, please use **either** the visually or programatical method to validate the cleaning was succesful.\n",
    "\n",
    "At this stage, you are also expected to remove variables that are unnecessary for your analysis and combine your datasets. Depending on your datasets, you may choose to perform variable combination and elimination before or after the cleaning stage. Your dataset must have **at least** 4 variables after combining the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_hourly_copy = df_hourly.copy()\n",
    "df_noaa_copy = df_noaa.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmFhN52Yyn3l"
   },
   "source": [
    "### **Quality Issue 1:**\n",
    "Setup as a proper date format and abstract out a year and month column for later grouping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9UejDWrNMW4a"
   },
   "source": [
    "df_noaa_copy.loc[:, \"year\"] = pd.Series(\n",
    "    [dt[1].split(\"-\")[0] for dt in df_noaa_copy[\"DATE\"].items()]\n",
    ")\n",
    "df_noaa_copy.loc[:, \"month\"] = pd.Series(\n",
    "    [dt[1].split(\"-\")[1] for dt in df_noaa_copy[\"DATE\"].items()]\n",
    ")\n",
    "df_noaa_copy.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure data integrity prior to conversion\n",
    "int_cols = ['DP01','DP10','DP1X','DSND','DSNW','DT00','DT32','DX32','DX70','DX90','DYFG','DYHF','DYNT','DYSD','DYSN','DYTS','DYXP','DYXT','WDF2','WDF5']\n",
    "float_cols = ['CDSD','CLDD','EMNT','EMSD','EMSN','EMXP','EMXT','HDSD','HTDD','PRCP','SNOW','TAVG','TMAX','TMIN','WSF2','WSF5']\n",
    "\n",
    "for col in int_cols:\n",
    "    if col in df_noaa_copy.columns:\n",
    "        df_noaa_copy[col] = pd.to_numeric(df_noaa_copy[col], errors='coerce').astype('Int64')\n",
    "\n",
    "for col in float_cols:\n",
    "    if col in df_noaa_copy.columns:\n",
    "        df_noaa_copy[col] = pd.to_numeric(df_noaa_copy[col], errors='coerce').astype(float)\n",
    "\n",
    "# Ensure date is datetime64[ns] representing the first day of the month\n",
    "df_noaa_copy['date'] = pd.to_datetime(df_noaa_copy['DATE'], format='%Y-%m', errors='raise')\n",
    "\n",
    "# Inspect dtypes after coercion\n",
    "df_noaa_copy.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_DAUbJrymBL"
   },
   "source": [
    "### **Quality Issue 2**\n",
    "Identifying null columns and dropping ones that exceed a reasonable threshold. If a pattern develops by year we could shorten the time span for comparision."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Ensure we filter out the _ATTRIBUTE columns to shorten the width of this df\n",
    "df_noaa_copy = df_noaa_copy[[col for col in df_noaa_copy.columns if not col.endswith('_ATTRIBUTES')]]\n",
    "# Count total rows per year (denominator)\n",
    "rows_per_year = df_noaa_copy.groupby(\"year\").size()\n",
    "\n",
    "# Sum of nulls per year by selected columns\n",
    "nulls_by_year = df_noaa_copy.groupby(\"year\")[df_noaa_copy.columns].apply(\n",
    "    lambda g: g.isnull().sum()\n",
    ")\n",
    "\n",
    "# Convert to proportions\n",
    "null_proportion_by_year = (nulls_by_year.div(rows_per_year, axis=0) * 100).round(1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "ax = sns.heatmap(\n",
    "    null_proportion_by_year,\n",
    "    annot=False,\n",
    "    cmap='Greens',\n",
    "    vmin=0, vmax=100,\n",
    "    linewidths=0.5, linecolor='white',\n",
    "    cbar_kws={'label': '% null', 'format': '%.0f%%'}\n",
    ")\n",
    "plt.title('Proportion Nulls by Year (Top Null Columns)')\n",
    "plt.ylabel('Year')\n",
    "plt.xlabel('Column')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Using the heatmap above we can visually see where some fields are null and at which year they are concentrated.\n",
    "# To clean up this chart we filter out columns that won't be useful for analysis, and include fields of interest\n",
    "try:\n",
    "    df_noaa_copy.drop(['ADPT', 'ASLP', 'ASTP', 'AWBT', 'AWND', 'PSUN', 'RHAV', 'RHMN', 'RHMX', 'TSUN', 'WDF1', 'WDFG', 'WDFM', 'WSF1', 'WSFG', 'WSFM'], axis=1, inplace=True)\n",
    "except KeyError:\n",
    "    # Already dropped these columns\n",
    "    pass\n",
    "rows_per_year = df_noaa_copy.groupby(\"year\").size()\n",
    "nulls_by_year = df_noaa_copy.groupby(\"year\")[df_noaa_copy.columns].apply(\n",
    "    lambda g: g.isnull().sum()\n",
    ")\n",
    "null_proportion_by_year = (nulls_by_year.div(rows_per_year, axis=0) * 100).round(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ionB2sRaMUmY"
   },
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "ax = sns.heatmap(\n",
    "    null_proportion_by_year,\n",
    "    annot=False,\n",
    "    cmap='Greens',\n",
    "    vmin=0, vmax=100,\n",
    "    linewidths=0.5, linecolor='white',\n",
    "    cbar_kws={'label': '% null', 'format': '%.0f%%'}\n",
    ")\n",
    "plt.title('Proportion Nulls by Year (Top Null Columns)')\n",
    "plt.ylabel('Year')\n",
    "plt.xlabel('Column')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now the Heatmap is looking a lot cleaner. We will have to keep in mind there are some gaps with the WDF fields, but since it is bound by date we can work around that."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIUrrfSNyOPR"
   },
   "source": [
    "### **Tidiness Issue 1**\n",
    "Prior to storing we will want to bring the location data into normal form via the Location table found in models.py"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_noaa_copy.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We can remove the station\n",
    "current_columns = df_noaa_copy.columns\n",
    "if 'STATION' in current_columns:\n",
    "    df_noaa_copy.drop('STATION', axis=1, inplace=True)\n",
    "if 'LONGITUDE' in current_columns:\n",
    "    df_noaa_copy.drop('LONGITUDE', axis=1, inplace=True)\n",
    "if 'LATITUDE' in current_columns:\n",
    "    df_noaa_copy.drop('LATITUDE', axis=1, inplace=True)\n",
    "if 'NAME' in current_columns:\n",
    "    df_noaa_copy.drop('NAME', axis=1, inplace=True)\n",
    "if 'ELEVATION' in current_columns:\n",
    "    df_noaa_copy.drop('ELEVATION', axis=1, inplace=True)\n",
    "if 'location_id' not in current_columns:\n",
    "    df_noaa_copy.loc[:, 'location_id'] = 1\n",
    "df_noaa_copy.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Justification: By simplifying down to a normal form location table we can expand this to join to other data sets by location and date in the future."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o51Bt8kwyTzk"
   },
   "source": [
    "### **Tidiness Issue 2**\n",
    "In order to compare monthly data we will need to build up a weekly and then monthly summary with the Open Meteo dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clear out any existing records in the daily_weather table\n",
    "DailyWeatherRecord.__table__.drop(ENGINE, checkfirst=True)\n",
    "DailyWeatherRecord.__table__.create(ENGINE, checkfirst=True)\n",
    "df_hourly_copy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_hourly_copy[\"day\"] = df_hourly_copy[\"date\"].dt.normalize()\n",
    "df_hourly_copy.head()\n",
    "# Add in proper date formatting"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by day and compute aggregates\n",
    "df_agg_daily = (\n",
    "    df_hourly_copy.groupby(\"day\").agg(\n",
    "        average_temperature=(\"temperature\", \"mean\"),\n",
    "        min_temperature=(\"temperature\", \"min\"),\n",
    "        max_temperature=(\"temperature\", \"max\"),\n",
    "        average_wind_speed=(\"wind_speed\", \"mean\"),\n",
    "        min_wind_speed=(\"wind_speed\", \"min\"),\n",
    "        max_wind_speed=(\"wind_speed\", \"max\"),\n",
    "        precipitation_sum=(\"precipitation\", \"sum\"),\n",
    "        precipitation_min=(\"precipitation\", \"min\"),\n",
    "        precipitation_max=(\"precipitation\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"day\": \"date_time\"})\n",
    ")\n",
    "df_agg_daily.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add calendar columns and location_id\n",
    "df_agg_daily[\"month\"] = df_agg_daily[\"date_time\"].dt.month.astype(int)\n",
    "df_agg_daily[\"day_of_month\"] = df_agg_daily[\"date_time\"].dt.day.astype(int)\n",
    "df_agg_daily[\"year\"] = df_agg_daily[\"date_time\"].dt.year.astype(int)\n",
    "df_agg_daily[\"location_id\"] = 1 # Current default location is 1 for Lander, Wyoming"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q6I_Sr7lxXi5"
   },
   "source": "df_agg_daily.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove unnecessary variables and combine datasets**\n",
    "\n",
    "Depending on the datasets, you can also peform the combination before the cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Unnecessary variables and fields were removed prior to this step",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F42urHuzttjF"
   },
   "source": [
    "## 4. Update your data store\n",
    "Update your local database/data store with the cleaned data, following best practices for storing your cleaned data:\n",
    "\n",
    "- Must maintain different instances / versions of data (raw and cleaned data)\n",
    "- Must name the dataset files informatively\n",
    "- Ensure both the raw and cleaned data is saved to your database/data store"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V3uay7EJUV_L"
   },
   "source": [
    "# Ensure column ordering matches models.py file\n",
    "agg_df = df_agg_daily[\n",
    "    [\n",
    "        \"location_id\",\n",
    "        \"date_time\",\n",
    "        \"month\",\n",
    "        \"day_of_month\",\n",
    "        \"year\",\n",
    "        \"average_temperature\",\n",
    "        \"min_temperature\",\n",
    "        \"max_temperature\",\n",
    "        \"average_wind_speed\",\n",
    "        \"min_wind_speed\",\n",
    "        \"max_wind_speed\",\n",
    "        \"precipitation_sum\",\n",
    "        \"precipitation_min\",\n",
    "        \"precipitation_max\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "records = agg_df.to_dict(orient=\"records\")\n",
    "stmt = insert(DailyWeatherRecord)\n",
    "with ENGINE.begin() as conn:\n",
    "    for record in records:\n",
    "        conn.execute(stmt, record)\n",
    "    conn.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "NOAAStationMonthlySummary.__table__.drop(ENGINE, checkfirst=True)\n",
    "NOAAStationMonthlySummary.__table__.create(ENGINE, checkfirst=True)\n",
    "df_noaa_final = df_noaa_copy[\n",
    "    ['location_id', 'date', 'CDSD', 'CLDD', 'DP01', 'DP10', 'DP1X', 'DSND', 'DSNW', 'DT00',\n",
    "       'DT32', 'DX32', 'DX70', 'DX90', 'DYFG', 'DYHF', 'DYNT', 'DYSD', 'DYSN',\n",
    "       'DYTS', 'DYXP', 'DYXT', 'EMNT', 'EMSD', 'EMSN', 'EMXP', 'EMXT', 'HDSD',\n",
    "       'HTDD', 'PRCP', 'SNOW', 'TAVG', 'TMAX', 'TMIN', 'WDF2', 'WDF5', 'WSF2',\n",
    "       'WSF5']\n",
    "]\n",
    "\n",
    "records = df_noaa_final.to_dict(orient=\"records\")\n",
    "stmt = insert(NOAAStationMonthlySummary)\n",
    "with ENGINE.begin() as conn:\n",
    "    for record in records:\n",
    "        conn.execute(stmt, record)\n",
    "    conn.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGy_yddGtzhM"
   },
   "source": [
    "## 5. Answer the research question\n",
    "\n",
    "### **5.1:** Define and answer the research question \n",
    "Going back to the problem statement in step 1, use the cleaned data to answer the question you raised. Produce **at least** two visualizations using the cleaned data and explain how they help you answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjedE4s4ZkEd"
   },
   "source": [
    "*Research question:* FILL IN from answer to Step 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lkw3rW9kZmOm"
   },
   "source": [
    "# Visual 1 - Year-over-Year change in TMAX (annual mean)\n",
    "_df_noaa = pd.read_sql_table('noaa_monthly_summary', ENGINE)\n",
    "# Ensure date is datetime\n",
    "a = _df_noaa\n",
    "if not pd.api.types.is_datetime64_any_dtype(a.get('date')):\n",
    "    a['date'] = pd.to_datetime(a['date'])\n",
    "# Compute annual mean TMAX\n",
    "a['year'] = a['date'].dt.year.astype(int)\n",
    "annual_tmax = (\n",
    "    a.groupby('year', as_index=False)['TMAX']\n",
    "     .mean()\n",
    "     .rename(columns={'TMAX': 'tmax_mean'})\n",
    ")\n",
    "# Year-over-year absolute change\n",
    "annual_tmax['yoy_change'] = annual_tmax['tmax_mean'].diff()\n",
    "# Plot YoY change as a bar chart with up/down colors\n",
    "colors = ['green' if v >= 0 else 'red' for v in annual_tmax['yoy_change'].fillna(0)]\n",
    "ax = annual_tmax.plot(x='year', y='yoy_change', kind='bar', color=colors, legend=False, figsize=(18, 5))\n",
    "ax.set_title('Year-over-Year Change in TMAX (Annual Mean)')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Change in TMAX')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to research question:*\n",
    "As we can see there is some variability year over year, but the average maximum temperature has been trending upwards in Lander Wyoming. Next let's see by how much over each decade."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6fdK_8ZGZm9R"
   },
   "source": [
    "# Visual 2 - Annual average temperature trend (TAVG) with 10-year rolling mean and linear trend\n",
    "annual_tavg = (\n",
    "    _df_noaa.groupby('year', as_index=False)['TAVG']\n",
    "      .mean()\n",
    "      .rename(columns={'TAVG': 'tavg_mean'})\n",
    "      .sort_values('year')\n",
    ")\n",
    "# 10-year rolling mean (centered)\n",
    "annual_tavg['roll10'] = annual_tavg['tavg_mean'].rolling(10, min_periods=5, center=True).mean()\n",
    "# Linear trend\n",
    "import numpy as np\n",
    "x = annual_tavg['year'].to_numpy()\n",
    "y = annual_tavg['tavg_mean'].to_numpy()\n",
    "if len(annual_tavg) >= 2:\n",
    "    coef = np.polyfit(x, y, 1)\n",
    "    trend_fn = np.poly1d(coef)\n",
    "    annual_tavg['trend'] = trend_fn(x)\n",
    "    slope_decade = coef[0] * 10.0\n",
    "else:\n",
    "    annual_tavg['trend'] = np.nan\n",
    "    slope_decade = float('nan')\n",
    "# Plot\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(annual_tavg['year'], annual_tavg['tavg_mean'], color='lightgray', linewidth=2, label='Annual mean (TAVG)')\n",
    "plt.plot(annual_tavg['year'], annual_tavg['roll10'], color='steelblue', linewidth=3, label='10-yr rolling mean')\n",
    "if np.isfinite(slope_decade):\n",
    "    plt.plot(annual_tavg['year'], annual_tavg['trend'], color='crimson', linestyle='--', linewidth=2, label='Linear trend')\n",
    "    plt.title('Annual Average Temperature (TAVG) with 10-year Rolling Mean and Linear Trend')\n",
    "else:\n",
    "    plt.title('Annual Average Temperature (TAVG) with 10-year Rolling Mean')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "if np.isfinite(slope_decade):\n",
    "    print(f'Estimated warming trend: {slope_decade:.2f} °C per decade')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5RgvMGUZoHn"
   },
   "source": [
    "*Answer to research question:*\n",
    "To further suggest the increase in temperature year over year we can see in the above chart that the average temperature has been trending hotter by around 0.1 degrees C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ezWXXZVj-TP"
   },
   "source": [
    "### **5.2:** Reflection\n",
    "In 2-4 sentences, if you had more time to complete the project, what actions would you take? For example, which data quality and structural issues would you look into further, and what research questions would you further explore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB3RBDG5kFe1"
   },
   "source": [
    "*Answer:*\n",
    "Given more time I would like to investigate the trends as of late, and how they compare to events around the world. For example hurricanes, Cloud Seeding, and changes in emissions output or policy.\n",
    "Given more time I would like to investigate further trends in the wide assortment of variables present in the Open Meteo, and NOAA data sets. By having these sets parsed aggregated and in a local database I willb e able to build on this project as time goes on."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
